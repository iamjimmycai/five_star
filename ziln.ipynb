{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeBA5OMLKW14WQQSZc4ACa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamjimmycai/five_star/blob/main/ziln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import LogNormal\n",
        "\n",
        "def zero_inflated_lognormal_pred(logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"计算零膨胀对数正态分布的预测均值。\n",
        "\n",
        "    参数:\n",
        "        logits: [batch_size, 3] 的 logits 张量。\n",
        "\n",
        "    返回:\n",
        "        preds: [batch_size, 1] 的预测均值张量。\n",
        "    \"\"\"\n",
        "    logits = logits.float()\n",
        "    positive_probs = torch.sigmoid(logits[..., :1])\n",
        "    loc = logits[..., 1:2]\n",
        "    scale = F.softplus(logits[..., 2:])\n",
        "    preds = positive_probs * torch.exp(loc + 0.5 * torch.square(scale))\n",
        "    return preds\n",
        "\n",
        "def zero_inflated_lognormal_loss(labels: torch.Tensor,\n",
        "                                 logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"计算零膨胀对数正态分布的损失。\n",
        "\n",
        "    参数:\n",
        "        labels: 真实目标，形状为 [batch_size, 1] 的张量。\n",
        "        logits: 输出层的 logits，形状为 [batch_size, 3] 的张量。\n",
        "\n",
        "    返回:\n",
        "        零膨胀对数正态分布的损失值。\n",
        "    \"\"\"\n",
        "    labels = labels.float()\n",
        "    positive = (labels > 0).float()\n",
        "\n",
        "    logits = logits.float()\n",
        "    positive_logits = logits[..., :1]\n",
        "    classification_loss = F.binary_cross_entropy_with_logits(positive_logits, positive)\n",
        "\n",
        "    loc = logits[..., 1:2]\n",
        "    scale = torch.maximum(F.softplus(logits[..., 2:]), torch.sqrt(torch.tensor(torch.finfo(torch.float32).eps)))\n",
        "    safe_labels = positive * labels + (1 - positive) * torch.ones_like(labels)\n",
        "\n",
        "    regression_loss = -torch.mean(positive * LogNormal(loc, scale).log_prob(safe_labels), dim=-1)\n",
        "\n",
        "    return classification_loss + regression_loss\n"
      ],
      "metadata": {
        "id": "k3pOIbZOvKIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from typing import Sequence\n",
        "\n",
        "def cumulative_true(y_true: Sequence[float], y_pred: Sequence[float]) -> np.ndarray:\n",
        "    \"\"\"计算根据预测排序的生命周期值的累积和。\n",
        "\n",
        "    参数:\n",
        "        y_true: 真实的生命周期值。\n",
        "        y_pred: 预测的生命周期值。\n",
        "\n",
        "    返回:\n",
        "        res: 根据预测排序的生命周期值的累积和。\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "    }).sort_values(by='y_pred', ascending=False)\n",
        "\n",
        "    return (df['y_true'].cumsum() / df['y_true'].sum()).values\n",
        "\n",
        "def gini_from_gain(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"计算增益图的Gini系数。\n",
        "\n",
        "    参数:\n",
        "        df: 每列包含一个增益图。第一列必须是真实值。\n",
        "\n",
        "    返回:\n",
        "        gini_result: 包含原始和标准化Gini系数的两列数据框。\n",
        "    \"\"\"\n",
        "    raw = df.apply(lambda x: 2 * x.sum() / df.shape[0] - 1.)\n",
        "    normalized = raw / raw[0]\n",
        "    return pd.DataFrame({\n",
        "        'raw': raw,\n",
        "        'normalized': normalized\n",
        "    })[['raw', 'normalized']]\n",
        "\n",
        "def _normalized_rmse(y_true, y_pred):\n",
        "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred)) / y_true.mean()\n",
        "\n",
        "def _normalized_mae(y_true, y_pred):\n",
        "    return metrics.mean_absolute_error(y_true, y_pred) / y_true.mean()\n",
        "\n",
        "def _aggregate_fn(df):\n",
        "    return pd.Series({\n",
        "        'label_mean': np.mean(df['y_true']),\n",
        "        'pred_mean': np.mean(df['y_pred']),\n",
        "        'normalized_rmse': _normalized_rmse(df['y_true'], df['y_pred']),\n",
        "        'normalized_mae': _normalized_mae(df['y_true'], df['y_pred']),\n",
        "    })\n",
        "\n",
        "def decile_stats(y_true: Sequence[float], y_pred: Sequence[float]) -> pd.DataFrame:\n",
        "    \"\"\"计算分位数级别的均值和误差。\n",
        "\n",
        "    该函数首先根据排序后的 `y_pred` 将样本划分为十个等大小的桶，并计算每个桶中的聚合指标。\n",
        "\n",
        "    参数:\n",
        "        y_true: 真实标签。\n",
        "        y_pred: 预测标签。\n",
        "\n",
        "    返回:\n",
        "        df: 分位数级别的统计数据。\n",
        "    \"\"\"\n",
        "    num_buckets = 10\n",
        "    decile = pd.qcut(y_pred, q=num_buckets, labels=['%d' % i for i in range(num_buckets)])\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "        'decile': decile,\n",
        "    }).groupby('decile').apply(_aggregate_fn)\n",
        "\n",
        "    df['decile_mape'] = np.abs(df['pred_mean'] - df['label_mean']) / df['label_mean']\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "oOsSYSaWvdxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-4tUIuLtwLL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import lifetime_value as ltv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn import model_selection, preprocessing\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from .utilities import *\n",
        "\n",
        "def preprocess(data, target_ltv, day1_purchaseAmt_col=\"\", numerical_features=[], categorical_features=[], sample_data=1, testing_size=0.2):\n",
        "    \"\"\"\n",
        "    数据预处理\n",
        "    \"\"\"\n",
        "    if isinstance(data, str):\n",
        "        path, file_type = os.path.splitext(data)\n",
        "        if file_type == \".csv\":\n",
        "            data = pd.read_csv(data)\n",
        "        elif file_type == \".parquet\":\n",
        "            data = pd.read_parquet(data, engine='pyarrow')\n",
        "\n",
        "    if 0 > sample_data or sample_data > 1:\n",
        "        raise ValueError(\"Error - `sample`必须是0和1之间的值，表示用于建模的数据百分比。默认使用100%的数据\")\n",
        "\n",
        "    if 0 >= testing_size or testing_size >= 1:\n",
        "        raise ValueError(\"Error - `testing_size`必须是0和1之间的值，表示用于测试的数据百分比。默认使用20%\")\n",
        "\n",
        "    data = data.sample(frac=sample_data, random_state=123)\n",
        "\n",
        "    feature_map = {}\n",
        "\n",
        "    if categorical_features == []:\n",
        "        categorical_features = data.select_dtypes([\"object\"]).columns.tolist()\n",
        "\n",
        "    if numerical_features == []:\n",
        "        numerical_features = data.select_dtypes([\"number\"]).columns.tolist()\n",
        "        numerical_features.remove(target_ltv)\n",
        "\n",
        "    feature_map[\"categorical_features\"] = categorical_features\n",
        "    feature_map[\"numerical_features\"] = numerical_features\n",
        "    feature_map[\"target\"] = target_ltv\n",
        "    feature_map[\"day1_purchaseAmt_col\"] = day1_purchaseAmt_col\n",
        "\n",
        "    data = data[categorical_features + numerical_features + [target_ltv, day1_purchaseAmt_col]]\n",
        "\n",
        "    if data[target_ltv].dtype != \"float32\":\n",
        "        data[target_ltv] = data[target_ltv].astype(\"float32\")\n",
        "\n",
        "    for col in categorical_features:\n",
        "        encoder = preprocessing.LabelEncoder()\n",
        "        encoder.fit(data[col])\n",
        "        levels = encoder.classes_\n",
        "        feature_map[col] = {levels[i]: i for i in range(len(levels))}\n",
        "        data[col] = encoder.transform(data[col])\n",
        "\n",
        "    y0 = data[day1_purchaseAmt_col].values\n",
        "\n",
        "    if day1_purchaseAmt_col not in numerical_features:\n",
        "        print(day1_purchaseAmt_col + \"未包含在numerical_features中，已移除\")\n",
        "        del data[day1_purchaseAmt_col]\n",
        "\n",
        "    train, test, y0_train, y0_test = model_selection.train_test_split(data, y0, test_size=testing_size, random_state=123)\n",
        "    x_train = feature_dict(train, numerical_features, categorical_features)\n",
        "    y_train = train[target_ltv].values\n",
        "\n",
        "    x_test = feature_dict(test, numerical_features, categorical_features)\n",
        "    y_test = test[target_ltv].values\n",
        "\n",
        "    return feature_map, x_train, x_test, y_train, y_test, y0_test\n",
        "\n",
        "class DNNModel(nn.Module):\n",
        "    def __init__(self, feature_map, layers):\n",
        "        super(DNNModel, self).__init__()\n",
        "        self.numeric_features = feature_map[\"numerical_features\"]\n",
        "        self.categorical_features = feature_map[\"categorical_features\"]\n",
        "\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(len(feature_map[key]), embedding_dim=10)  # 你可以根据需要调整embedding_dim\n",
        "            for key in self.categorical_features\n",
        "        ])\n",
        "        input_dim = len(self.numeric_features) + len(self.categorical_features) * 10  # 假设embedding_dim为10\n",
        "        self.dnn_layers = nn.Sequential(\n",
        "            *[nn.Linear(input_dim, i) for i in layers],\n",
        "            nn.Linear(layers[-1], 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, numeric_input, categorical_inputs):\n",
        "        embeddings = [embedding(input) for embedding, input in zip(self.embeddings, categorical_inputs)]\n",
        "        embeddings = torch.cat(embeddings, dim=1)\n",
        "        deep_input = torch.cat([numeric_input, embeddings], dim=1)\n",
        "        output = self.dnn_layers(deep_input)\n",
        "        return output\n",
        "\n",
        "def fit_model(feature_map, x_train, y_train, x_test, y_test,\n",
        "              layers=[64, 32, 3], epochs=100, learning_rate=0.0001, batch_size=1024,\n",
        "              callback_patience=20, callback_lr=1e-06, verbose=0, log_directory_name=\"logs\"):\n",
        "\n",
        "    model = DNNModel(feature_map, layers)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = ltv.zero_inflated_lognormal_loss  # 替换为你定义的损失函数\n",
        "\n",
        "    train_dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_dataset = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_x[:, :len(feature_map[\"numerical_features\"])],\n",
        "                           [batch_x[:, i] for i in range(len(feature_map[\"categorical_features\"]))])\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_predict(model, data, feature_map, print_performance=True):\n",
        "    if isinstance(data, str):\n",
        "        path, file_type = os.path.splitext(data)\n",
        "        if file_type == \".csv\":\n",
        "            data = pd.read_csv(data)\n",
        "        elif file_type == \".parquet\":\n",
        "            data = pd.read_parquet(data, engine='pyarrow')\n",
        "\n",
        "    all_variables = feature_map[\"categorical_features\"] + feature_map[\"numerical_features\"] + [feature_map[\"target\"], feature_map[\"day1_purchaseAmt_col\"]]\n",
        "\n",
        "    for col in all_variables:\n",
        "        if col not in data.columns:\n",
        "            raise ValueError(\"Error -\" + col + \" 列在`data`中未找到。请保持所有列名与建模时使用的列名一致\")\n",
        "\n",
        "    data = data[all_variables]\n",
        "    if data[feature_map[\"target\"]].dtype != \"float32\":\n",
        "        data[feature_map[\"target\"]] = data[feature_map[\"target\"]].astype(\"float32\")\n",
        "\n",
        "    for cat in feature_map[\"categorical_features\"]:\n",
        "        levels = list(feature_map[cat].keys())\n",
        "        data[cat] = data[cat].apply(lambda t: t if t in levels else 'UNDEFINED')\n",
        "        data[cat] = data[cat].apply(lambda t: feature_map[cat][t])\n",
        "\n",
        "    y0 = data[feature_map[\"day1_purchaseAmt_col\"]].values\n",
        "\n",
        "    x_test = feature_dict(data, feature_map[\"numerical_features\"], feature_map[\"categorical_features\"])\n",
        "    x_test = {feat: torch.tensor(np.array(x_test[feat])) for feat in x_test.keys()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x_test['numeric'], [x_test[key] for key in feature_map[\"categorical_features\"]])\n",
        "\n",
        "    ltv_pred = ltv.zero_inflated_lognormal_pred(logits).numpy().flatten()\n",
        "    churn_predictions = torch.sigmoid(logits[..., :1]).numpy().flatten()\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'churn_predictions': churn_predictions,\n",
        "        'ltv_prediction': ltv_pred\n",
        "    })\n",
        "\n",
        "    if print_performance:\n",
        "        metrics, preds = ltv_performance(model, x_test, data[feature_map[\"target\"]].values, y0)\n",
        "        print(metrics.transpose())\n",
        "\n",
        "    return df\n"
      ]
    }
  ]
}